Running DPO and RLHF on the Hopper-v3 environment in OpenAI Gym to learn how to make hops that move forward.
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb1aac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from network import MLP, MixtureOfExperts, SimpleCNN, SimpleLSTMClassifier\n",
    "from operations import mse_loss, cross_entropy, softmax, relu\n",
    "from tensor import Tensor\n",
    "from optimizers import SOAP, Adam\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from layers import Sequential, ReLU\n",
    "\n",
    "class TensorElement:\n",
    "    \"\"\"Adapter for a single scalar element in a Tensor so that updating it updates the parent Tensor.\"\"\"\n",
    "    def __init__(self, parent, i, j):\n",
    "        self.parent = parent\n",
    "        self.i = i\n",
    "        self.j = j\n",
    "\n",
    "    @property\n",
    "    def data(self):\n",
    "        return self.parent.data[self.i, self.j]\n",
    "\n",
    "    @data.setter\n",
    "    def data(self, value):\n",
    "        self.parent.data[self.i, self.j] = value\n",
    "\n",
    "    @property\n",
    "    def grad(self):\n",
    "        if self.parent.grad is None:\n",
    "            return None\n",
    "        return self.parent.grad[self.i, self.j]\n",
    "\n",
    "    @grad.setter\n",
    "    def grad(self, value):\n",
    "        if self.parent.grad is None:\n",
    "            self.parent.grad = np.zeros_like(self.parent.data)\n",
    "        self.parent.grad[self.i, self.j] = value\n",
    "\n",
    "def get_linear_weights(module):\n",
    "    \"\"\"Recursively collects weight Tensors from modules that have a 'weight' attribute.\"\"\"\n",
    "    weights = []\n",
    "    if hasattr(module, 'weight'):\n",
    "        weights.append(module.weight)\n",
    "    if hasattr(module, '_modules'):\n",
    "        for m in module._modules.values():\n",
    "            weights.extend(get_linear_weights(m))\n",
    "    return weights\n",
    "\n",
    "def tensor_to_matrix(tensor):\n",
    "    \"\"\"Converts a Tensor representing a matrix into a 2D list of TensorElement adapters.\"\"\"\n",
    "    m, n = tensor.data.shape\n",
    "    matrix = []\n",
    "    for i in range(m):\n",
    "        row = []\n",
    "        for j in range(n):\n",
    "            row.append(TensorElement(tensor, i, j))\n",
    "        matrix.append(row)\n",
    "    return matrix\n",
    "\n",
    "def zero_model_grad(module):\n",
    "    \"\"\"Zeros gradients for parameters of the module recursively.\"\"\"\n",
    "    for param in getattr(module, '_parameters', {}).values():\n",
    "        if param is not None:\n",
    "            param.grad = None\n",
    "    for sub in getattr(module, '_modules', {}).values():\n",
    "        zero_model_grad(sub)\n",
    "\n",
    "def update_biases(module, lr):\n",
    "    \"\"\"Updates biases using a simple SGD step if they exist.\"\"\"\n",
    "    if hasattr(module, 'bias') and module.bias.grad is not None:\n",
    "        module.bias.data -= lr * module.bias.grad\n",
    "    if hasattr(module, '_modules'):\n",
    "        for m in module._modules.values():\n",
    "            update_biases(m, lr)\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "# Download MNIST from openml (70000 samples)\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "X = mnist.data.astype(np.float32)\n",
    "y = mnist.target.astype(int)\n",
    "# Normalize and split\n",
    "X = X / 255.0\n",
    "# First 60k for training, last 10k for test\n",
    "x_train = X[:60000].T\n",
    "y_train = y[:60000]\n",
    "x_test  = X[60000:].T\n",
    "y_test  = y[60000:]\n",
    "# One-hot encode labels\n",
    "num_classes = 10\n",
    "y_train_oh = np.eye(num_classes)[y_train].T\n",
    "y_test_oh  = np.eye(num_classes)[y_test].T\n",
    "\n",
    "n_epochs = 2\n",
    "batch_size = 64\n",
    "lr = 0.01\n",
    "\n",
    "def run_experiment(model, name):\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    soap_opts = [SOAP(tensor_to_matrix(w), lr=lr) for w in get_linear_weights(model)]\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        perm = np.random.permutation(x_train.shape[1])\n",
    "        epoch_loss = 0.0\n",
    "        for i in range(0, perm.size, batch_size):\n",
    "            idx = perm[i:i+batch_size]\n",
    "            Xb = x_train[:, idx]\n",
    "            Yb = y_train_oh[:, idx]\n",
    "            zero_model_grad(model)\n",
    "            logits = model(Tensor(Xb, requires_grad=False))\n",
    "            probs = softmax(logits)\n",
    "            loss = cross_entropy(probs, Tensor(Yb, requires_grad=False))\n",
    "            loss.backward()\n",
    "            update_biases(model, lr)\n",
    "            for opt in soap_opts:\n",
    "                opt.step()\n",
    "                opt.zero_grad()\n",
    "            epoch_loss += loss.data\n",
    "        epoch_loss /= (perm.size / batch_size)\n",
    "        logits_test = model(Tensor(x_test, requires_grad=False))\n",
    "        preds_test = np.argmax(softmax(logits_test).data, axis=0)\n",
    "        acc = np.mean(preds_test == y_test)\n",
    "        print(f\"Epoch {epoch}/{n_epochs} — Loss: {epoch_loss:.4f}, Test Acc: {acc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8266571a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MLP ===\n",
      "Epoch 1/2 — Loss: 0.3830, Test Acc: 0.9322\n",
      "Epoch 2/2 — Loss: 0.2222, Test Acc: 0.9459\n"
     ]
    }
   ],
   "source": [
    "# Vanilla MLP\n",
    "mlp = MLP(in_features=784, hidden_features=[32, 16], out_features=num_classes)\n",
    "run_experiment(mlp, \"MLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7867f447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TwoLayerMoE ===\n",
      "Epoch 1/2 — Loss: 0.7344, Test Acc: 0.8925\n",
      "Epoch 2/2 — Loss: 0.3779, Test Acc: 0.8712\n"
     ]
    }
   ],
   "source": [
    "# Two-layer MoE\n",
    "moe_seq = Sequential(\n",
    "    MixtureOfExperts(input_dim=784, output_dim=32, num_specialized=4, num_shared=1, k=1, gamma=0.1),\n",
    "    ReLU(),\n",
    "    MixtureOfExperts(input_dim=32, output_dim=16, num_specialized=4, num_shared=1, k=1, gamma=0.1),\n",
    "    ReLU(),\n",
    "    MixtureOfExperts(input_dim=16, output_dim=num_classes, num_specialized=4, num_shared=1, k=2, gamma=0.1),\n",
    ")\n",
    "run_experiment(moe_seq, \"TwoLayerMoE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "639750f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SimpleCNN ===\n",
      "Epoch 1/2 — Loss: 0.2315, Test Acc: 0.9710\n",
      "Epoch 2/2 — Loss: 0.0817, Test Acc: 0.9789\n"
     ]
    }
   ],
   "source": [
    "# CNN\n",
    "def run_experiment_cnn(model, name):\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    opt = Adam(model.parameters(), lr=0.001)\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        perm = np.random.permutation(x_train.shape[1])\n",
    "        epoch_loss = 0.0\n",
    "        for i in range(0, perm.size, batch_size):\n",
    "            idx = perm[i:i+batch_size]\n",
    "            Xb = x_train[:, idx]  # (784, B)\n",
    "            Yb = y_train_oh[:, idx]\n",
    "            # reshape to (1, 28, 28, B)\n",
    "            B = Xb.shape[1]\n",
    "            Xb_img = Xb.T.reshape(B, 1, 28, 28).transpose(1, 2, 3, 0)\n",
    "            # forward\n",
    "            logits = model(Tensor(Xb_img, requires_grad=False))\n",
    "            probs = softmax(logits)\n",
    "            loss = cross_entropy(probs, Tensor(Yb, requires_grad=False))\n",
    "            # backward + step\n",
    "            for p in model.parameters():\n",
    "                p.grad = None\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            epoch_loss += loss.data\n",
    "        epoch_loss /= (perm.size / batch_size)\n",
    "        # test\n",
    "        Btest = x_test.shape[1]\n",
    "        Xtest_img = x_test.T.reshape(Btest, 1, 28, 28).transpose(1, 2, 3, 0)\n",
    "        logits_test = model(Tensor(Xtest_img, requires_grad=False))\n",
    "        preds_test = np.argmax(softmax(logits_test).data, axis=0)\n",
    "        acc = np.mean(preds_test == y_test)\n",
    "        print(f\"Epoch {epoch}/{n_epochs} — Loss: {epoch_loss:.4f}, Test Acc: {acc:.4f}\")\n",
    "\n",
    "cnn = SimpleCNN(num_classes=num_classes)\n",
    "run_experiment_cnn(cnn, \"SimpleCNN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd7dd204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SimpleLSTMClassifier ===\n",
      "Epoch 1/4 — Loss: 0.6646, Test Acc: 0.6120\n",
      "Epoch 2/4 — Loss: 0.6510, Test Acc: 0.6360\n",
      "Epoch 3/4 — Loss: 0.6355, Test Acc: 0.6700\n",
      "Epoch 4/4 — Loss: 0.6170, Test Acc: 0.7160\n"
     ]
    }
   ],
   "source": [
    "# RNN (LSTM) on synthetic sequence classification\n",
    "def make_sequence_dataset(num_samples=2000, T=8, input_dim=4):\n",
    "    X = np.random.randn(num_samples, input_dim, T)\n",
    "    # label: 1 if sum over first feature > 0 else 0\n",
    "    sums = X[:, 0, :].sum(axis=1)\n",
    "    y = (sums > 0).astype(int)\n",
    "    y_oh = np.eye(2)[y]\n",
    "    return X, y, y_oh\n",
    "\n",
    "X_seq_train, y_seq_train, y_seq_train_oh = make_sequence_dataset(num_samples=2000)\n",
    "X_seq_test, y_seq_test, y_seq_test_oh = make_sequence_dataset(num_samples=500)\n",
    "\n",
    "def run_experiment_lstm(model, name, T=8, input_dim=4):\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    opt = Adam(model.parameters(), lr=0.001)\n",
    "    Bsz = 32\n",
    "    epochs = 4\n",
    "    N = X_seq_train.shape[0]\n",
    "    for epoch in range(1, epochs+1):\n",
    "        perm = np.random.permutation(N)\n",
    "        epoch_loss = 0.0\n",
    "        for i in range(0, N, Bsz):\n",
    "            idx = perm[i:i+Bsz]\n",
    "            Xb = X_seq_train[idx]  # (B, input_dim, T)\n",
    "            Yb = y_seq_train_oh[idx]  # (B, 2)\n",
    "            # reshape to (input_dim, T, B)\n",
    "            Xb_t = Xb.transpose(1, 2, 0)\n",
    "            Yb_t = Yb.T\n",
    "            for p in model.parameters():\n",
    "                p.grad = None\n",
    "            logits = model(Tensor(Xb_t, requires_grad=False))\n",
    "            probs = softmax(logits)\n",
    "            loss = cross_entropy(probs, Tensor(Yb_t, requires_grad=False))\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            epoch_loss += loss.data\n",
    "        epoch_loss /= (N / Bsz)\n",
    "        # test\n",
    "        Xt = X_seq_test.transpose(1, 2, 0)\n",
    "        logits_test = model(Tensor(Xt, requires_grad=False))\n",
    "        preds = np.argmax(softmax(logits_test).data, axis=0)\n",
    "        acc = np.mean(preds == y_seq_test)\n",
    "        print(f\"Epoch {epoch}/{epochs} — Loss: {epoch_loss:.4f}, Test Acc: {acc:.4f}\")\n",
    "\n",
    "lstm = SimpleLSTMClassifier(input_dim=4, hidden_dim=8, num_classes=2)\n",
    "run_experiment_lstm(lstm, \"SimpleLSTMClassifier\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

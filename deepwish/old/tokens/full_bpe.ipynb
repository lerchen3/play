{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import multiprocessing\n",
        "import subprocess\n",
        "\n",
        "# Configuration\n",
        "input_csv_path = \"input.csv\"  # Update this path as needed\n",
        "num_threads = multiprocessing.cpu_count()\n",
        "vocab_size = 50000  # Target vocabulary size\n",
        "min_frequency = 2   # Minimum frequency for pairs to be considered\n",
        "\n",
        "print(f'CPU threads available: {num_threads}')\n",
        "print(f'Target vocabulary size: {vocab_size}')\n",
        "print(f'Minimum pair frequency: {min_frequency}')\n",
        "\n",
        "# Load dataset\n",
        "print('\\nLoading dataset...')\n",
        "start_time = time.time()\n",
        "df = pd.read_csv(input_csv_path)\n",
        "print(f'Dataset loaded in {time.time() - start_time:.2f}s')\n",
        "print(f'Total examples: {len(df)}')\n",
        "\n",
        "# Write corpus to text file for C++ processing\n",
        "print('Writing corpus to file...')\n",
        "with open('full_corpus.txt', 'w', encoding='utf-8') as f:\n",
        "    for text in df['assistant']:\n",
        "        # Escape newlines in the text itself\n",
        "        f.write(str(text).replace('\\n', '\\\\n') + '\\n')\n",
        "\n",
        "print(f'Wrote {len(df)} documents to full_corpus.txt')\n",
        "del df  # Free memory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile complete_bpe.cpp\n",
        "#include <bits/stdc++.h>\n",
        "#include <thread>\n",
        "#include <mutex>\n",
        "#include <atomic>\n",
        "using namespace std;\n",
        "\n",
        "struct PairCount {\n",
        "    long long pair;\n",
        "    long long count;\n",
        "    int first_token, second_token;\n",
        "};\n",
        "\n",
        "class CompleteBPE {\n",
        "private:\n",
        "    vector<vector<int>> docs;\n",
        "    int num_threads;\n",
        "    int vocab_size;\n",
        "    int min_frequency;\n",
        "    int current_vocab_id;\n",
        "    mutex mtx;\n",
        "    \n",
        "    // Vocabulary tracking\n",
        "    map<int, string> vocab; // token_id -> string representation\n",
        "    map<int, pair<int, int>> merges; // token_id -> (first_token, second_token) for merged tokens\n",
        "    \n",
        "public:\n",
        "    CompleteBPE(const string& corpus_file, int threads, int target_vocab, int min_freq) \n",
        "        : num_threads(threads), vocab_size(target_vocab), min_frequency(min_freq), current_vocab_id(256) {\n",
        "        initializeVocab();\n",
        "        loadCorpus(corpus_file);\n",
        "    }\n",
        "    \n",
        "    void initializeVocab() {\n",
        "        // Initialize byte-level vocabulary (0-255)\n",
        "        for(int i = 0; i < 256; ++i) {\n",
        "            if(i == 0) {\n",
        "                vocab[i] = \"<NULL>\";\n",
        "            } else if(i == 9) {\n",
        "                vocab[i] = \"<TAB>\";\n",
        "            } else if(i == 10) {\n",
        "                vocab[i] = \"<LF>\";\n",
        "            } else if(i == 13) {\n",
        "                vocab[i] = \"<CR>\";\n",
        "            } else if(i == 32) {\n",
        "                vocab[i] = \"<SPACE>\";\n",
        "            } else if(i >= 33 && i <= 126) {\n",
        "                // Printable ASCII\n",
        "                vocab[i] = string(1, (char)i);\n",
        "            } else {\n",
        "                // Non-printable bytes\n",
        "                vocab[i] = \"<BYTE_\" + to_string(i) + \">\";\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    void loadCorpus(const string& corpus_file) {\n",
        "        cout << \"Loading corpus...\" << endl;\n",
        "        auto start = chrono::high_resolution_clock::now();\n",
        "        \n",
        "        vector<string> lines;\n",
        "        ifstream fin(corpus_file);\n",
        "        string line;\n",
        "        while(getline(fin, line)) {\n",
        "            lines.push_back(line);\n",
        "        }\n",
        "        fin.close();\n",
        "        \n",
        "        int N = lines.size();\n",
        "        docs.resize(N);\n",
        "        \n",
        "        // Convert to token arrays in parallel\n",
        "        vector<thread> threads;\n",
        "        auto convert_worker = [&](int tid) {\n",
        "            int start = tid * N / num_threads;\n",
        "            int end = (tid + 1) * N / num_threads;\n",
        "            \n",
        "            for(int i = start; i < end; ++i) {\n",
        "                auto& s = lines[i];\n",
        "                auto& doc = docs[i];\n",
        "                doc.reserve(s.size());\n",
        "                for(char c : s) {\n",
        "                    doc.push_back((unsigned char)c);\n",
        "                }\n",
        "            }\n",
        "        };\n",
        "        \n",
        "        for(int t = 0; t < num_threads; ++t) {\n",
        "            threads.emplace_back(convert_worker, t);\n",
        "        }\n",
        "        for(auto& th : threads) {\n",
        "            th.join();\n",
        "        }\n",
        "        \n",
        "        auto end = chrono::high_resolution_clock::now();\n",
        "        double load_time = chrono::duration<double>(end - start).count();\n",
        "        cout << \"Corpus loaded: \" << N << \" documents in \" << load_time << \"s\" << endl;\n",
        "    }\n",
        "    \n",
        "    unordered_map<long long, long long> countPairs() {\n",
        "        unordered_map<long long, long long> global_counts;\n",
        "        global_counts.reserve(1000000);\n",
        "        \n",
        "        vector<thread> threads;\n",
        "        auto count_worker = [&](int tid) {\n",
        "            int N = docs.size();\n",
        "            int start = tid * N / num_threads;\n",
        "            int end = (tid + 1) * N / num_threads;\n",
        "            unordered_map<long long, long long> local_counts;\n",
        "            \n",
        "            for(int i = start; i < end; ++i) {\n",
        "                auto& tokens = docs[i];\n",
        "                for(int j = 0; j + 1 < tokens.size(); ++j) {\n",
        "                    long long key = ((long long)tokens[j] << 32) | (unsigned int)tokens[j+1];\n",
        "                    local_counts[key]++;\n",
        "                }\n",
        "            }\n",
        "            \n",
        "            lock_guard<mutex> lock(mtx);\n",
        "            for(auto& kv : local_counts) {\n",
        "                global_counts[kv.first] += kv.second;\n",
        "            }\n",
        "        };\n",
        "        \n",
        "        for(int t = 0; t < num_threads; ++t) {\n",
        "            threads.emplace_back(count_worker, t);\n",
        "        }\n",
        "        for(auto& th : threads) {\n",
        "            th.join();\n",
        "        }\n",
        "        \n",
        "        return global_counts;\n",
        "    }\n",
        "    \n",
        "    vector<PairCount> getBatchMerges(const unordered_map<long long, long long>& counts) {\n",
        "        // Convert to vector and sort by frequency\n",
        "        vector<PairCount> candidates;\n",
        "        for(auto& kv : counts) {\n",
        "            if(kv.second >= min_frequency) {\n",
        "                PairCount pc;\n",
        "                pc.pair = kv.first;\n",
        "                pc.count = kv.second;\n",
        "                pc.first_token = (int)(kv.first >> 32);\n",
        "                pc.second_token = (int)(kv.first & 0xFFFFFFFF);\n",
        "                candidates.push_back(pc);\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        sort(candidates.begin(), candidates.end(), [](const PairCount& a, const PairCount& b) {\n",
        "            return a.count > b.count;\n",
        "        });\n",
        "        \n",
        "        // Greedily select non-conflicting pairs, limiting to 100 per batch\n",
        "        vector<PairCount> batch;\n",
        "        unordered_set<int> used_tokens;\n",
        "        \n",
        "        for(const auto& candidate : candidates) {\n",
        "            // Stop if we've reached the batch limit\n",
        "            if(batch.size() >= 100) break;\n",
        "            \n",
        "            // Check for conflicts: xy,yz or xy,zx\n",
        "            bool conflict = false;\n",
        "            \n",
        "            for(const auto& existing : batch) {\n",
        "                // Check xy,yz pattern (second token of existing = first token of candidate)\n",
        "                if(existing.second_token == candidate.first_token) {\n",
        "                    conflict = true;\n",
        "                    break;\n",
        "                }\n",
        "                // Check xy,zx pattern (first token of existing = second token of candidate)\n",
        "                if(existing.first_token == candidate.second_token) {\n",
        "                    conflict = true;\n",
        "                    break;\n",
        "                }\n",
        "                // Check zx,xy pattern (second token of candidate = first token of existing)\n",
        "                if(candidate.second_token == existing.first_token) {\n",
        "                    conflict = true;\n",
        "                    break;\n",
        "                }\n",
        "                // Check yz,xy pattern (first token of candidate = second token of existing)\n",
        "                if(candidate.first_token == existing.second_token) {\n",
        "                    conflict = true;\n",
        "                    break;\n",
        "                }\n",
        "            }\n",
        "            \n",
        "            if(!conflict) {\n",
        "                batch.push_back(candidate);\n",
        "                used_tokens.insert(candidate.first_token);\n",
        "                used_tokens.insert(candidate.second_token);\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        return batch;\n",
        "    }\n",
        "    \n",
        "    void applyMerges(const vector<PairCount>& merges) {\n",
        "        if(merges.empty()) return;\n",
        "        \n",
        "        unordered_map<long long, int> merge_map;\n",
        "        for(const auto& merge : merges) {\n",
        "            int new_token_id = current_vocab_id++;\n",
        "            merge_map[merge.pair] = new_token_id;\n",
        "            \n",
        "            // Track the merge in vocabulary\n",
        "            this->merges[new_token_id] = {merge.first_token, merge.second_token};\n",
        "            \n",
        "            // Create string representation for the merged token\n",
        "            string first_str = vocab[merge.first_token];\n",
        "            string second_str = vocab[merge.second_token];\n",
        "            vocab[new_token_id] = first_str + second_str;\n",
        "        }\n",
        "        \n",
        "        vector<thread> threads;\n",
        "        auto merge_worker = [&](int tid) {\n",
        "            int N = docs.size();\n",
        "            int start = tid * N / num_threads;\n",
        "            int end = (tid + 1) * N / num_threads;\n",
        "            \n",
        "            for(int i = start; i < end; ++i) {\n",
        "                auto& tokens = docs[i];\n",
        "                vector<int> new_tokens;\n",
        "                new_tokens.reserve(tokens.size());\n",
        "                \n",
        "                for(size_t j = 0; j < tokens.size(); ) {\n",
        "                    if(j + 1 < tokens.size()) {\n",
        "                        long long key = ((long long)tokens[j] << 32) | (unsigned int)tokens[j+1];\n",
        "                        auto it = merge_map.find(key);\n",
        "                        if(it != merge_map.end()) {\n",
        "                            new_tokens.push_back(it->second);\n",
        "                            j += 2;\n",
        "                            continue;\n",
        "                        }\n",
        "                    }\n",
        "                    new_tokens.push_back(tokens[j]);\n",
        "                    j++;\n",
        "                }\n",
        "                tokens.swap(new_tokens);\n",
        "            }\n",
        "        };\n",
        "        \n",
        "        for(int t = 0; t < num_threads; ++t) {\n",
        "            threads.emplace_back(merge_worker, t);\n",
        "        }\n",
        "        for(auto& th : threads) {\n",
        "            th.join();\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    void runCompleteBPE() {\n",
        "        cout << \"\\nStarting complete BPE process...\" << endl;\n",
        "        auto start_total = chrono::high_resolution_clock::now();\n",
        "        \n",
        "        int iteration = 0;\n",
        "        \n",
        "        while(current_vocab_id < vocab_size) {\n",
        "            iteration++;\n",
        "            cout << \"\\n--- Iteration \" << iteration << \" ---\" << endl;\n",
        "            cout << \"Current vocab size: \" << current_vocab_id << endl;\n",
        "            \n",
        "            // Count pairs\n",
        "            auto start_count = chrono::high_resolution_clock::now();\n",
        "            auto counts = countPairs();\n",
        "            auto end_count = chrono::high_resolution_clock::now();\n",
        "            double count_time = chrono::duration<double>(end_count - start_count).count();\n",
        "            \n",
        "            cout << \"Pair counting: \" << count_time << \"s (\" << counts.size() << \" unique pairs)\" << endl;\n",
        "            \n",
        "            // Get batch merges\n",
        "            auto start_batch = chrono::high_resolution_clock::now();\n",
        "            auto batch = getBatchMerges(counts);\n",
        "            auto end_batch = chrono::high_resolution_clock::now();\n",
        "            double batch_time = chrono::duration<double>(end_batch - start_batch).count();\n",
        "            \n",
        "            if(batch.empty()) {\n",
        "                cout << \"No more pairs to merge (min frequency: \" << min_frequency << \")\" << endl;\n",
        "                break;\n",
        "            }\n",
        "            \n",
        "            cout << \"Batch selection: \" << batch_time << \"s (\" << batch.size() << \" merges)\" << endl;\n",
        "            \n",
        "            // Apply merges\n",
        "            auto start_merge = chrono::high_resolution_clock::now();\n",
        "            applyMerges(batch);\n",
        "            auto end_merge = chrono::high_resolution_clock::now();\n",
        "            double merge_time = chrono::duration<double>(end_merge - start_merge).count();\n",
        "            \n",
        "            cout << \"Merge application: \" << merge_time << \"s\" << endl;\n",
        "            \n",
        "            // Show top merges\n",
        "            cout << \"Top merges in this batch:\" << endl;\n",
        "            for(int i = 0; i < min(5, (int)batch.size()); ++i) {\n",
        "                cout << \"  (\" << batch[i].first_token << \", \" << batch[i].second_token \n",
        "                     << \") -> \" << (current_vocab_id - batch.size() + i) \n",
        "                     << \" (count: \" << batch[i].count << \")\" << endl;\n",
        "            }\n",
        "            \n",
        "            if(current_vocab_id >= vocab_size) {\n",
        "                cout << \"Reached target vocabulary size!\" << endl;\n",
        "                break;\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        auto end_total = chrono::high_resolution_clock::now();\n",
        "        double total_time = chrono::duration<double>(end_total - start_total).count();\n",
        "        \n",
        "        cout << \"\\n\" << string(50, '=') << endl;\n",
        "        cout << \"BPE COMPLETE!\" << endl;\n",
        "        cout << \"Final vocabulary size: \" << current_vocab_id << endl;\n",
        "        cout << \"Total iterations: \" << iteration << endl;\n",
        "        cout << \"Total time: \" << total_time << \"s\" << endl;\n",
        "        cout << \"Threads used: \" << num_threads << endl;\n",
        "        cout << string(50, '=') << endl;\n",
        "        \n",
        "        // Save vocabulary to CSV\n",
        "        saveVocabulary();\n",
        "    }\n",
        "    \n",
        "    void saveVocabulary() {\n",
        "        cout << \"\\nSaving vocabulary to vocab.csv...\" << endl;\n",
        "        auto start = chrono::high_resolution_clock::now();\n",
        "        \n",
        "        ofstream fout(\"vocab.csv\");\n",
        "        fout << \"token_id,token_string,is_merged,first_token,second_token\\n\";\n",
        "        \n",
        "        for(int i = 0; i < current_vocab_id; ++i) {\n",
        "            fout << i << \",\\\"\";\n",
        "            \n",
        "            // Escape quotes in the token string\n",
        "            string token_str = vocab[i];\n",
        "            for(char c : token_str) {\n",
        "                if(c == '\"') {\n",
        "                    fout << \"\\\"\\\"\";\n",
        "                } else {\n",
        "                    fout << c;\n",
        "                }\n",
        "            }\n",
        "            fout << \"\\\"\";\n",
        "            \n",
        "            if(merges.count(i)) {\n",
        "                // This is a merged token\n",
        "                fout << \",true,\" << merges[i].first << \",\" << merges[i].second;\n",
        "            } else {\n",
        "                // This is a byte-level token\n",
        "                fout << \",false,,\";\n",
        "            }\n",
        "            fout << \"\\n\";\n",
        "        }\n",
        "        \n",
        "        fout.close();\n",
        "        \n",
        "        auto end = chrono::high_resolution_clock::now();\n",
        "        double save_time = chrono::duration<double>(end - start).count();\n",
        "        \n",
        "        cout << \"Vocabulary saved: \" << current_vocab_id << \" tokens in \" << save_time << \"s\" << endl;\n",
        "        cout << \"Output file: vocab.csv\" << endl;\n",
        "    }\n",
        "};\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "    if(argc < 5) {\n",
        "        cerr << \"Usage: ./complete_bpe corpus.txt num_threads vocab_size min_frequency\" << endl;\n",
        "        return 1;\n",
        "    }\n",
        "    \n",
        "    string corpus_file = argv[1];\n",
        "    int num_threads = stoi(argv[2]);\n",
        "    int vocab_size = stoi(argv[3]);\n",
        "    int min_frequency = stoi(argv[4]);\n",
        "    \n",
        "    CompleteBPE bpe(corpus_file, num_threads, vocab_size, min_frequency);\n",
        "    bpe.runCompleteBPE();\n",
        "    \n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile the complete BPE implementation\n",
        "print('Compiling complete BPE C++ code...')\n",
        "start_compile = time.time()\n",
        "\n",
        "result = subprocess.run([\n",
        "    'g++', '-O3', '-march=native', '-std=c++17', '-pthread', \n",
        "    'complete_bpe.cpp', '-o', 'complete_bpe'\n",
        "], capture_output=True, text=True)\n",
        "\n",
        "compile_time = time.time() - start_compile\n",
        "\n",
        "if result.returncode != 0:\n",
        "    print('Compilation failed:')\n",
        "    print(result.stderr)\n",
        "    print(result.stdout)\n",
        "else:\n",
        "    print(f'Compilation successful! ({compile_time:.2f}s)')\n",
        "    print('Optimizations: -O3 -march=native')\n",
        "    print('Features: Multi-threading, batch merging, conflict detection')\n",
        "\n",
        "start_bpe = time.time()\n",
        "\n",
        "# Run the complete BPE process\n",
        "result = subprocess.run([\n",
        "    './complete_bpe', \n",
        "    'full_corpus.txt', \n",
        "    str(num_threads), \n",
        "    str(vocab_size), \n",
        "    str(min_frequency)\n",
        "], capture_output=True, text=True)\n",
        "\n",
        "total_bpe_time = time.time() - start_bpe\n",
        "\n",
        "print(result.stdout)\n",
        "if result.stderr:\n",
        "    print('STDERR:', result.stderr)\n",
        "\n",
        "print(f'\\nWall clock time for complete BPE: {total_bpe_time:.2f}s')\n",
        "print(f'Average time per 1000 vocab tokens: {total_bpe_time / (vocab_size/1000):.2f}s')\n",
        "\n",
        "# Performance summary and analysis\n",
        "print('\\n' + '=' * 60)\n",
        "print('PERFORMANCE SUMMARY')\n",
        "print('=' * 60)\n",
        "print(f'Implementation: Multi-threaded C++ with batch merging')\n",
        "print(f'CPU threads utilized: {num_threads}')\n",
        "print(f'Dataset size: {len(pd.read_csv(input_csv_path))} documents')\n",
        "print(f'Target vocabulary: {vocab_size} tokens')\n",
        "print(f'Minimum frequency threshold: {min_frequency}')\n",
        "print(f'Total processing time: {total_bpe_time:.2f}s')\n",
        "print(f'Throughput: {vocab_size/total_bpe_time:.1f} vocab tokens/second')\n",
        "print('\\nFeatures implemented:')\n",
        "print('✓ Multi-threaded corpus loading and processing')\n",
        "print('✓ Parallel pair counting across documents')\n",
        "print('✓ Batch merging with conflict detection (xy,yz patterns)')\n",
        "print('✓ Greedy selection by frequency')\n",
        "print('✓ Parallel merge application')\n",
        "print('✓ Complete BPE process until target vocabulary')\n",
        "print('=' * 60)\n",
        "\n",
        "# Run the complete BPE process\n",
        "print('=' * 60)\n",
        "print('RUNNING COMPLETE BPE PROCESS ON FULL DATASET')\n",
        "print('=' * 60)\n",
        "print(f'Dataset: {input_csv_path}')\n",
        "print(f'Threads: {num_threads}')\n",
        "print(f'Target vocab size: {vocab_size}')\n",
        "print(f'Min frequency: {min_frequency}')\n",
        "print('=' * 60)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

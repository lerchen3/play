{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaL4","dataSources":[{"sourceId":91496,"databundleVersionId":11802066,"sourceType":"competition"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import triton\nprint(triton.__version__)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-30T02:43:57.261444Z","iopub.execute_input":"2025-06-30T02:43:57.261716Z","iopub.status.idle":"2025-06-30T02:43:57.889864Z","shell.execute_reply.started":"2025-06-30T02:43:57.261698Z","shell.execute_reply":"2025-06-30T02:43:57.889324Z"}},"outputs":[{"name":"stdout","text":"3.2.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nprint(torch.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T02:43:57.890795Z","iopub.execute_input":"2025-06-30T02:43:57.890974Z","iopub.status.idle":"2025-06-30T02:44:02.497303Z","shell.execute_reply.started":"2025-06-30T02:43:57.890959Z","shell.execute_reply":"2025-06-30T02:44:02.496740Z"}},"outputs":[{"name":"stdout","text":"2.6.0+cu124\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%%writefile blah.py\nimport torch\nimport torch.nn as nn\nimport torch.distributed as dist\nimport os\nimport sys\nimport time\nimport argparse\n\ndef setup_distributed():\n    \"\"\"Initialize distributed training.\"\"\"\n    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n        rank = int(os.environ['RANK'])\n        world_size = int(os.environ['WORLD_SIZE'])\n        local_rank = int(os.environ.get('LOCAL_RANK', 0))\n        \n        # Initialize process group\n        dist.init_process_group(backend='nccl')\n        torch.cuda.set_device(local_rank)\n        \n        return rank, world_size, local_rank\n    else:\n        # Single GPU mode\n        return 0, 1, 0\n\ndef cleanup_distributed():\n    \"\"\"Clean up distributed training.\"\"\"\n    if dist.is_initialized():\n        dist.destroy_process_group()\n\ndef get_gpu_memory_usage():\n    \"\"\"Returns the currently used GPU memory in GiB.\"\"\"\n    # Report memory for the current CUDA device\n    current = torch.cuda.current_device()\n    return torch.cuda.memory_allocated(current) / (1024**3)\n\nclass BigModel(nn.Module):\n    \"\"\"A large model with multiple layers to test checkpointing.\"\"\"\n    def __init__(self, hidden_dim, num_layers, bottleneck_dim=None):\n        super().__init__()\n        # Use a smaller bottleneck to balance params vs activations\n        if bottleneck_dim is None:\n            bottleneck_dim = hidden_dim // 2\n        self.bottleneck_dim = bottleneck_dim\n        # Each layer: down-project then up-project\n        self.layers = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(hidden_dim, bottleneck_dim),\n                nn.ReLU(),\n                nn.Linear(bottleneck_dim, hidden_dim),\n                nn.ReLU()\n            )\n            for _ in range(num_layers)\n        ])\n        # Add a simple head for final output\n        self.head = nn.Parameter(torch.randn(hidden_dim, hidden_dim) / (hidden_dim ** 0.5))\n\n    def forward(self, x):\n        # This is a standard forward pass. The custom training loop will handle checkpointing.\n        for idx, layer in enumerate(self.layers):\n            x = layer(x)\n            print(f\"[Vanilla] After layer {idx}, GPU memory: {get_gpu_memory_usage():.2f} GiB\")\n        return x @ self.head\n\ndef run_distributed_offload_step(model, input_tensor, target_tensor, args, adam_states, step):\n    \"\"\"\n    Implements the exact same distributed offloading procedure as train_fa2_cce_offload.py\n    \"\"\"\n    device = next(model.parameters()).device\n    betas = (args.adam_beta1, args.adam_beta2)\n    eps = args.adam_eps\n    \n    # Get distributed training info\n    world_size = dist.get_world_size() if dist.is_initialized() else 1\n    rank = dist.get_rank() if dist.is_initialized() else 0\n    # local print only on rank 0\n    def _print(*pmsg, **pkwargs):\n        if rank == 0:\n            print(*pmsg, **pkwargs)\n\n    def _get_param_shard(param, rank, world_size):\n        \"\"\"Get the shard of parameters this rank should handle.\"\"\"\n        total_elements = param.numel()\n        elements_per_rank = total_elements // world_size\n        start_idx = rank * elements_per_rank\n        end_idx = start_idx + elements_per_rank if rank < world_size - 1 else total_elements\n        return start_idx, end_idx\n\n    def _apply_adam_update_shard(param_shard, m_shard, v_shard, grad_shard, t, lr, betas, eps, weight_decay):\n        \"\"\"Applies Adam update to a parameter shard.\"\"\"\n        beta1, beta2 = betas\n        \n        m_shard.mul_(beta1).add_(grad_shard, alpha=1 - beta1)\n        v_shard.mul_(beta2).addcmul_(grad_shard, grad_shard, value=1 - beta2)\n        \n        m_hat = m_shard / (1 - beta1 ** t)\n        v_hat = v_shard / (1 - beta2 ** t)\n        \n        if weight_decay > 0:\n            param_shard.add_(param_shard, alpha=-lr * weight_decay)\n\n        param_shard.addcdiv_(m_hat, v_hat.sqrt().add_(eps), value=-lr)\n        return m_shard, v_shard\n\n    def _reduce_scatter_gradients(params):\n        \"\"\"Reduce-scatter gradients across GPUs.\"\"\"\n        for p in params:\n            if p.grad is None:\n                continue\n            \n            # Flatten gradient\n            grad_flat = p.grad.data.view(-1)\n            total_elements = grad_flat.numel()\n            \n            if world_size > 1:\n                # Create output tensor for this rank's shard\n                elements_per_rank = total_elements // world_size\n                start_idx = rank * elements_per_rank\n                end_idx = start_idx + elements_per_rank if rank < world_size - 1 else total_elements\n                shard_size = end_idx - start_idx\n                \n                # Prepare input list for reduce_scatter\n                input_list = [grad_flat[i * elements_per_rank:(i + 1) * elements_per_rank] \n                             for i in range(world_size - 1)]\n                # Handle last shard which might be larger\n                input_list.append(grad_flat[(world_size - 1) * elements_per_rank:])\n                \n                # Output tensor for this rank's portion\n                output = torch.zeros(shard_size, device=device, dtype=grad_flat.dtype)\n                \n                # Reduce-scatter operation\n                dist.reduce_scatter(output, input_list, op=dist.ReduceOp.SUM)\n                \n                # Store the reduced shard back\n                p.grad_shard = output\n            else:\n                p.grad_shard = grad_flat\n\n    def _all_gather_params(params):\n        \"\"\"All-gather parameters after updates.\"\"\"\n        for p in params:\n            if world_size > 1:\n                # Get the parameter shards from all ranks\n                param_flat = p.data.view(-1)\n                total_elements = param_flat.numel()\n                elements_per_rank = total_elements // world_size\n                \n                # Each rank contributes its shard\n                start_idx = rank * elements_per_rank\n                end_idx = start_idx + elements_per_rank if rank < world_size - 1 else total_elements\n                my_shard = param_flat[start_idx:end_idx]\n                \n                # Create list to gather into - each element should be the size of a shard\n                gather_list = []\n                for i in range(world_size):\n                    if i < world_size - 1:\n                        shard_size = elements_per_rank\n                    else:\n                        shard_size = total_elements - i * elements_per_rank\n                    gather_list.append(torch.zeros(shard_size, device=device, dtype=param_flat.dtype))\n                \n                # All-gather operation\n                dist.all_gather(gather_list, my_shard)\n                \n                # Reconstruct full parameter\n                full_param = torch.cat(gather_list)\n                p.data.copy_(full_param.view(p.data.shape))\n    \n    modules = list(model.layers)\n    offloaded = []\n    transfer_stream = torch.cuda.Stream()\n    \n    x = input_tensor\n    _print(f\"[Distributed] Initial GPU memory: {get_gpu_memory_usage():.2f} GiB\")\n\n    # Forward pass with activation offloading\n    with torch.no_grad():\n        for m in modules:\n            with torch.cuda.stream(transfer_stream):\n                buf = x.to('cpu', non_blocking=True).pin_memory()                    \n            ev_act = torch.cuda.Event()\n            ev_act.record(transfer_stream)\n            offloaded.append((buf, ev_act, m))\n            x = m(x)\n            _print(f\"[Distributed] After forward layer, GPU memory: {get_gpu_memory_usage():.2f} GiB\")\n\n    torch.cuda.empty_cache()\n    \n    hidden = x.clone().detach().requires_grad_(True)\n\n    # Loss computation\n    output = hidden @ model.head\n    loss_main = nn.MSELoss()(output, target_tensor)\n    _print(f\"[Distributed] After loss calc, GPU memory: {get_gpu_memory_usage():.2f} GiB\")\n    \n    total_loss = loss_main\n    \n    # Backward pass for head\n    total_loss.backward()\n\n    # Reduce-scatter and update head parameters\n    with torch.no_grad():\n        head_params = [model.head]\n        \n        # Reduce-scatter gradients\n        _reduce_scatter_gradients(head_params)\n        \n        # Update only this rank's shard\n        for p in head_params:\n            if not hasattr(p, 'grad_shard'):\n                continue\n                \n            state = adam_states[p]\n            start_idx, end_idx = _get_param_shard(p, rank, world_size)\n            \n            # Get CPU optimizer state for this shard\n            m_cpu_shard = state['m'].view(-1)[start_idx:end_idx]\n            v_cpu_shard = state['v'].view(-1)[start_idx:end_idx]\n            \n            # Move to GPU\n            m_gpu_shard = m_cpu_shard.to(device)\n            v_gpu_shard = v_cpu_shard.to(device)\n            \n            # Get parameter shard\n            param_shard = p.data.view(-1)[start_idx:end_idx]\n            \n            # Apply Adam update to shard\n            m_gpu_shard, v_gpu_shard = _apply_adam_update_shard(\n                param_shard, m_gpu_shard, v_gpu_shard, p.grad_shard, \n                step, args.lr, betas, eps, args.weight_decay\n            )\n            \n            # Move optimizer state back to CPU\n            state['m'].view(-1)[start_idx:end_idx].copy_(m_gpu_shard.to('cpu'))\n            state['v'].view(-1)[start_idx:end_idx].copy_(v_gpu_shard.to('cpu'))\n            \n            p.grad = None\n            delattr(p, 'grad_shard')\n        \n        # All-gather updated parameters\n        _all_gather_params(head_params)\n\n    grad = hidden.grad.clone()\n    del hidden, total_loss, loss_main\n    torch.cuda.empty_cache()\n\n    # Layer-wise backward with distributed updates\n    for buf, ev_act, m in reversed(offloaded):\n        torch.cuda.current_stream().wait_event(ev_act)\n        inp = buf.to(device, non_blocking=True)\n        if not torch.is_floating_point(inp):\n            inp.requires_grad = False\n        else:\n            inp.requires_grad = True\n\n        with torch.enable_grad():\n            out = m(inp)\n        \n        torch.autograd.backward(out, grad_tensors=[grad])\n        \n        # Prefetch optimizer states for this rank's parameter shards\n        for p in m.parameters():\n            start_idx, end_idx = _get_param_shard(p, rank, world_size)\n            cpu_state = adam_states[p]\n            # Only transfer the shard this rank needs\n            adam_states[p]['_m_gpu_shard'] = cpu_state['m'].view(-1)[start_idx:end_idx].to(device, non_blocking=True)\n            adam_states[p]['_v_gpu_shard'] = cpu_state['v'].view(-1)[start_idx:end_idx].to(device, non_blocking=True)\n            adam_states[p]['_shard_info'] = (start_idx, end_idx)\n        \n        # Reduce-scatter gradients for this layer\n        layer_params = list(m.parameters())\n        _reduce_scatter_gradients(layer_params)\n        \n        with torch.no_grad():\n            for p in layer_params:\n                if not hasattr(p, 'grad_shard'):\n                    continue\n                    \n                state = adam_states[p]\n                start_idx, end_idx = state['_shard_info']\n                \n                # Use prefetched optimizer state shards\n                m_gpu_shard = state.pop('_m_gpu_shard')\n                v_gpu_shard = state.pop('_v_gpu_shard')\n                \n                # Get parameter shard\n                param_shard = p.data.view(-1)[start_idx:end_idx]\n                \n                # Apply Adam update\n                m_gpu_shard, v_gpu_shard = _apply_adam_update_shard(\n                    param_shard, m_gpu_shard, v_gpu_shard, p.grad_shard,\n                    step, args.lr, betas, eps, args.weight_decay\n                )\n                \n                # Asynchronously move optimizer state back to CPU\n                with torch.cuda.stream(transfer_stream):\n                    state['m'].view(-1)[start_idx:end_idx].copy_(m_gpu_shard.to('cpu', non_blocking=True))\n                    state['v'].view(-1)[start_idx:end_idx].copy_(v_gpu_shard.to('cpu', non_blocking=True))\n                \n                delattr(p, 'grad_shard')\n                state.pop('_shard_info')\n        \n            # All-gather updated parameters for this layer\n            _all_gather_params(layer_params)\n            \n            m.zero_grad(set_to_none=True)\n\n        if inp.grad is not None:\n            grad = inp.grad.clone()\n\n        del inp, out, buf, ev_act\n        torch.cuda.empty_cache()\n        _print(f\"[Distributed] After layer backward cleanup, GPU memory: {get_gpu_memory_usage():.2f} GiB\")\n\n    final_loss = nn.MSELoss()(x.detach() @ model.head, target_tensor)\n    return final_loss.item()\n\ndef run_vanilla_training_step(model, input_tensor, target_tensor, optimizer, loss_fn):\n    \"\"\"Performs a standard PyTorch training step.\"\"\"\n    print(f\"[Vanilla] Before zero_grad: {get_gpu_memory_usage():.2f} GiB\")\n    optimizer.zero_grad()\n    print(f\"[Vanilla] After zero_grad: {get_gpu_memory_usage():.2f} GiB\")\n    output = model(input_tensor)\n    print(f\"[Vanilla] After forward: {get_gpu_memory_usage():.2f} GiB\")\n    loss = loss_fn(output, target_tensor)\n    print(f\"Loss: {loss}\")\n    print(f\"[Vanilla] After loss calc: {get_gpu_memory_usage():.2f} GiB\")\n    loss.backward()\n    print(f\"[Vanilla] After backward: {get_gpu_memory_usage():.2f} GiB\")\n    optimizer.step()\n    print(f\"[Vanilla] After optimizer.step: {get_gpu_memory_usage():.2f} GiB\")\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Test distributed offloading example\")\n    parser.add_argument('--hidden_dim', type=int, default=10240, help='Hidden dimension')\n    parser.add_argument('--num_layers', type=int, default=10, help='Number of layers')\n    parser.add_argument('--batch_size', type=int, default=10240, help='Batch size')\n    parser.add_argument('--lr', type=float, default=1e-4, help='Learning rate')\n    parser.add_argument('--weight_decay', type=float, default=0.01, help='Weight decay')\n    parser.add_argument('--adam_beta1', type=float, default=0.9, help='Adam beta1')\n    parser.add_argument('--adam_beta2', type=float, default=0.999, help='Adam beta2')\n    parser.add_argument('--adam_eps', type=float, default=1e-8, help='Adam epsilon')\n    parser.add_argument('--skip_vanilla', action='store_true', help='Skip vanilla benchmark')\n    return parser.parse_args()\n\ndef main():\n    # Initialize distributed training\n    rank, world_size, local_rank = setup_distributed()\n    device = torch.device(f'cuda:{local_rank}')\n    \n    try:\n        args = parse_args()\n        \n        # Only print from rank 0\n        def print_rank0(*msg):\n            if rank == 0:\n                print(*msg)\n\n        if not torch.cuda.is_available():\n            print_rank0(\"Skipping test: CUDA device not available.\")\n            return\n\n        # Configuration\n        HIDDEN_DIM = args.hidden_dim\n        BATCH_SIZE = args.batch_size\n        NUM_LAYERS = args.num_layers\n        BOTTLENECK_DIM = HIDDEN_DIM // 2\n        DEVICE = device\n\n        print_rank0(\"--- Test Configuration ---\")\n        print_rank0(f\"World Size: {world_size}, Rank: {rank}, Local Rank: {local_rank}\")\n        print_rank0(f\"Device: {torch.cuda.get_device_name(DEVICE)}\")\n        print_rank0(f\"Hidden Dim: {HIDDEN_DIM}, Bottleneck Dim: {BOTTLENECK_DIM}, Batch Size: {BATCH_SIZE}, Layers: {NUM_LAYERS}\\n\")\n        \n        # Expected memory calculations\n        exp_weight_mem = NUM_LAYERS * (HIDDEN_DIM * BOTTLENECK_DIM * 2) * 4 / (1024**3)\n        exp_act_mem = BATCH_SIZE * HIDDEN_DIM * 4 / (1024**3)\n        exp_vanilla_mem = exp_weight_mem + exp_act_mem * NUM_LAYERS\n        exp_custom_mem = exp_weight_mem + exp_act_mem\n        print_rank0(f\"Expected weights memory: {exp_weight_mem:.2f} GiB\")\n        print_rank0(f\"Expected activation memory per layer: {exp_act_mem:.2f} GiB\")\n        print_rank0(f\"Expected vanilla peak memory: {exp_vanilla_mem:.2f} GiB\")\n        print_rank0(f\"Expected distributed peak memory: {exp_custom_mem:.2f} GiB\")\n\n        # Create test data\n        input_tensor = torch.randn(BATCH_SIZE, HIDDEN_DIM, device=DEVICE)\n        target_tensor = torch.randn(BATCH_SIZE, HIDDEN_DIM, device=DEVICE)\n        loss_fn = nn.MSELoss()\n\n        # --- 1. Vanilla PyTorch Benchmark (only on rank 0 to avoid OOM) ---\n        if not args.skip_vanilla and rank == 0:\n            print_rank0(\"--- Running Vanilla PyTorch Benchmark ---\")\n            model_vanilla = BigModel(hidden_dim=HIDDEN_DIM, num_layers=NUM_LAYERS).to(DEVICE)\n            optimizer_vanilla = torch.optim.Adam(model_vanilla.parameters(), lr=args.lr)\n\n            try:\n                torch.cuda.synchronize()\n                torch.cuda.reset_peak_memory_stats(DEVICE)\n                start_time = time.time()\n\n                run_vanilla_training_step(model_vanilla, input_tensor, target_tensor, optimizer_vanilla, loss_fn)\n                \n                torch.cuda.synchronize()\n                end_time = time.time()\n                \n                peak_mem_vanilla = torch.cuda.max_memory_allocated(DEVICE) / (1024**3)\n                time_vanilla = end_time - start_time\n                print_rank0(\"âœ… Vanilla training step completed.\")\n                print_rank0(f\"â±ï¸ Time Taken: {time_vanilla:.4f} seconds\")\n                print_rank0(f\"ðŸ“ˆ Peak GPU Memory: {peak_mem_vanilla:.2f} GiB\")\n\n            except (torch.cuda.OutOfMemoryError, RuntimeError) as e:\n                print_rank0(f\"âŒ FAILED: Vanilla training failed with error: {e}\")\n            finally:\n                del model_vanilla, optimizer_vanilla\n                torch.cuda.empty_cache()\n\n            print_rank0(\"\\n\" + \"=\"*40 + \"\\n\")\n\n        # --- 2. Distributed Offloading Benchmark ---\n        print_rank0(\"--- Running Distributed Offloading Benchmark ---\")\n        model_distributed = BigModel(hidden_dim=HIDDEN_DIM, num_layers=NUM_LAYERS).to(DEVICE)\n        \n        # We manually shard parameters, no DDP wrapper needed\n        real_model = model_distributed\n        \n        # Store optimizer state on CPU pinned memory for async transfer\n        adam_states = {p: {'m': torch.zeros_like(p, device='cpu').pin_memory(),\n                          'v': torch.zeros_like(p, device='cpu').pin_memory()} for p in real_model.parameters()}\n        \n        try:\n            torch.cuda.synchronize()\n            torch.cuda.reset_peak_memory_stats(DEVICE)\n            start_time = time.time()\n\n            step = 1\n            loss_val = run_distributed_offload_step(real_model, input_tensor, target_tensor, args, adam_states, step)\n            \n            torch.cuda.synchronize()\n            end_time = time.time()\n\n            peak_mem_custom = torch.cuda.max_memory_allocated(DEVICE) / (1024**3)\n            time_custom = end_time - start_time\n            print_rank0(f\"\\nâœ… Distributed training step completed with loss: {loss_val:.6f}\")\n            print_rank0(f\"â±ï¸ Time Taken: {time_custom:.4f} seconds\")\n            print_rank0(f\"ðŸ“ˆ Peak GPU Memory: {peak_mem_custom:.2f} GiB\")\n\n        except (torch.cuda.OutOfMemoryError, RuntimeError) as e:\n            print_rank0(f\"âŒ FAILED: Distributed training failed with error: {e}\")\n        finally:\n            del model_distributed, input_tensor, target_tensor, loss_fn\n            torch.cuda.empty_cache()\n            print_rank0(f\"\\nFinal GPU Memory after cleanup: {get_gpu_memory_usage():.2f} GiB\")\n\n    finally:\n        cleanup_distributed()\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T02:44:02.498139Z","iopub.execute_input":"2025-06-30T02:44:02.498433Z","iopub.status.idle":"2025-06-30T02:44:02.510988Z","shell.execute_reply.started":"2025-06-30T02:44:02.498413Z","shell.execute_reply":"2025-06-30T02:44:02.510398Z"}},"outputs":[{"name":"stdout","text":"Writing blah.py\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!torchrun --nproc_per_node=4 blah.py --hidden_dim 8192 --num_layers 12","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T02:44:02.511571Z","iopub.execute_input":"2025-06-30T02:44:02.511748Z","iopub.status.idle":"2025-06-30T02:45:56.439934Z","shell.execute_reply.started":"2025-06-30T02:44:02.511728Z","shell.execute_reply":"2025-06-30T02:45:56.439272Z"}},"outputs":[{"name":"stdout","text":"W0630 02:44:04.368000 188 torch/distributed/run.py:792] \nW0630 02:44:04.368000 188 torch/distributed/run.py:792] *****************************************\nW0630 02:44:04.368000 188 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0630 02:44:04.368000 188 torch/distributed/run.py:792] *****************************************\n[W630 02:44:12.083220422 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W630 02:44:20.085364428 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W630 02:44:30.784090882 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W630 02:44:30.785761416 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W630 02:44:30.788971012 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W630 02:44:30.791136688 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W630 02:44:38.786346436 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W630 02:44:46.787730279 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W630 02:44:54.789501808 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W630 02:45:02.791949716 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n--- Test Configuration ---\nWorld Size: 4, Rank: 0, Local Rank: 0\nDevice: NVIDIA L4\nHidden Dim: 8192, Bottleneck Dim: 4096, Batch Size: 10240, Layers: 12\n\nExpected weights memory: 3.00 GiB\nExpected activation memory per layer: 0.31 GiB\nExpected vanilla peak memory: 6.75 GiB\nExpected distributed peak memory: 3.31 GiB\n--- Running Vanilla PyTorch Benchmark ---\n[Vanilla] Before zero_grad: 3.88 GiB\n[Vanilla] After zero_grad: 3.88 GiB\n[Vanilla] After layer 0, GPU memory: 4.35 GiB\n[Vanilla] After layer 1, GPU memory: 4.82 GiB\n[Vanilla] After layer 2, GPU memory: 5.29 GiB\n[Vanilla] After layer 3, GPU memory: 5.76 GiB\n[Vanilla] After layer 4, GPU memory: 6.23 GiB\n[Vanilla] After layer 5, GPU memory: 6.70 GiB\n[Vanilla] After layer 6, GPU memory: 7.16 GiB\n[Vanilla] After layer 7, GPU memory: 7.63 GiB\n[Vanilla] After layer 8, GPU memory: 8.10 GiB\n[Vanilla] After layer 9, GPU memory: 8.57 GiB\n[Vanilla] After layer 10, GPU memory: 9.04 GiB\n[Vanilla] After layer 11, GPU memory: 9.51 GiB\n[Vanilla] After forward: 9.82 GiB\nLoss: 1.0000078678131104\n[Vanilla] After loss calc: 10.13 GiB\n[Vanilla] After backward: 7.77 GiB\n[Vanilla] After optimizer.step: 14.27 GiB\nâœ… Vanilla training step completed.\nâ±ï¸ Time Taken: 5.0051 seconds\nðŸ“ˆ Peak GPU Memory: 17.52 GiB\n\n========================================\n\n--- Running Distributed Offloading Benchmark ---\n[Distributed] Initial GPU memory: 3.89 GiB\n[Distributed] After forward layer, GPU memory: 4.20 GiB\n[Distributed] After forward layer, GPU memory: 4.20 GiB\n[Distributed] After forward layer, GPU memory: 4.20 GiB\n[Distributed] After forward layer, GPU memory: 4.20 GiB\n[Distributed] After forward layer, GPU memory: 4.20 GiB\n[Distributed] After forward layer, GPU memory: 4.20 GiB\n[Distributed] After forward layer, GPU memory: 4.20 GiB\n[Distributed] After forward layer, GPU memory: 4.20 GiB\n[Distributed] After forward layer, GPU memory: 4.20 GiB\n[Distributed] After forward layer, GPU memory: 4.20 GiB\n[Distributed] After forward layer, GPU memory: 4.20 GiB\n[Distributed] After forward layer, GPU memory: 4.20 GiB\n[Distributed] After loss calc, GPU memory: 5.14 GiB\n[Distributed] After layer backward cleanup, GPU memory: 5.45 GiB\n[Distributed] After layer backward cleanup, GPU memory: 5.45 GiB\n[Distributed] After layer backward cleanup, GPU memory: 5.45 GiB\n[Distributed] After layer backward cleanup, GPU memory: 5.45 GiB\n[Distributed] After layer backward cleanup, GPU memory: 5.45 GiB\n[Distributed] After layer backward cleanup, GPU memory: 5.45 GiB\n[Distributed] After layer backward cleanup, GPU memory: 5.45 GiB\n[Distributed] After layer backward cleanup, GPU memory: 5.45 GiB\n[Distributed] After layer backward cleanup, GPU memory: 5.45 GiB\n[Distributed] After layer backward cleanup, GPU memory: 5.45 GiB\n[Distributed] After layer backward cleanup, GPU memory: 5.45 GiB\n[Distributed] After layer backward cleanup, GPU memory: 5.45 GiB\n\nâœ… Distributed training step completed with loss: 0.999984\nâ±ï¸ Time Taken: 11.4155 seconds\nðŸ“ˆ Peak GPU Memory: 7.08 GiB\n\nFinal GPU Memory after cleanup: 3.27 GiB\n","output_type":"stream"}],"execution_count":4}]}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-13T02:55:51.049615Z",
     "iopub.status.busy": "2025-06-13T02:55:51.049185Z",
     "iopub.status.idle": "2025-06-13T02:55:51.775095Z",
     "shell.execute_reply": "2025-06-13T02:55:51.774550Z",
     "shell.execute_reply.started": "2025-06-13T02:55:51.049597Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.0\n"
     ]
    }
   ],
   "source": [
    "import triton\n",
    "print(triton.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T02:55:51.775842Z",
     "iopub.status.busy": "2025-06-13T02:55:51.775663Z",
     "iopub.status.idle": "2025-06-13T02:55:56.848132Z",
     "shell.execute_reply": "2025-06-13T02:55:56.847603Z",
     "shell.execute_reply.started": "2025-06-13T02:55:51.775826Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T03:01:28.552016Z",
     "iopub.status.busy": "2025-06-13T03:01:28.551435Z",
     "iopub.status.idle": "2025-06-13T03:01:46.170454Z",
     "shell.execute_reply": "2025-06-13T03:01:46.169926Z",
     "shell.execute_reply.started": "2025-06-13T03:01:28.551993Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Conflicting meta-parameters: NUM_LOCKS, BLOCK_N, BLOCK_V. Make sure that you don't re-define auto-tuned symbols.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20207/3035793484.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m     \u001b[0mtest_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m     \u001b[0mtest_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0mbenchmark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshow_plots\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_20207/3035793484.py\u001b[0m in \u001b[0;36mtest_forward\u001b[0;34m()\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0mref_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;31m# triton loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0mtri_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtriton_cut_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_locks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtri_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Forward test failed: ref={ref_loss.item()}, tri={tri_loss.item()}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Forward test passed: {ref_loss.item():.6f} vs {tri_loss.item():.6f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_20207/3035793484.py\u001b[0m in \u001b[0;36mtriton_cut_cross_entropy\u001b[0;34m(E, C, x, block_size, block_n, num_locks)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;31m# Convenience module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtriton_cut_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_locks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTritonCCE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_locks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;31m# Numerical correctness tests for CCE kernel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_setup_ctx_defined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_20207/3035793484.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, E, C, x, block_size, block_n, num_locks)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mlocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_locks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtriton\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_n\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtriton\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         _cce_fwd_parallel_kernel[grid](\n\u001b[0m\u001b[1;32m    162\u001b[0m             \u001b[0mE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_corr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/triton/runtime/jit.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mmemorizes\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \"\"\"\n\u001b[0;32m--> 347\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarmup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0;31m# return cast(T, functools.partial(cast(Callable, self.run), grid=grid))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/triton/runtime/autotuner.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0mpruned_configs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprune_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0mbench_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m                 \u001b[0mtimings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bench\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpruned_configs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m                 \u001b[0mbench_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbench_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbench_end\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbench_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/triton/runtime/autotuner.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0mpruned_configs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprune_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0mbench_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m                 \u001b[0mtimings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bench\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpruned_configs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m                 \u001b[0mbench_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbench_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbench_end\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbench_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/triton/runtime/autotuner.py\u001b[0m in \u001b[0;36m_bench\u001b[0;34m(self, config, *args, **meta)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mconflicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconflicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             raise ValueError(f\"Conflicting meta-parameters: {', '.join(conflicts)}.\"\n\u001b[0m\u001b[1;32m    146\u001b[0m                              \" Make sure that you don't re-define auto-tuned symbols.\")\n\u001b[1;32m    147\u001b[0m         \u001b[0;31m# augment meta-parameters with tunable ones\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Conflicting meta-parameters: NUM_LOCKS, BLOCK_N, BLOCK_V. Make sure that you don't re-define auto-tuned symbols."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import triton.testing\n",
    "\n",
    "# --- Autotune configs for forward kernel ---\n",
    "_fwd_configs = [\n",
    "    triton.Config({'BLOCK_N': bn, 'BLOCK_V': bv, 'NUM_LOCKS': nl}, num_warps=nw, num_stages=1)\n",
    "    for bn in [8, 16, 32]\n",
    "    for bv in [64, 128, 256, 512]\n",
    "    for nl in [64, 128, 256]\n",
    "    for nw in [2, 4, 8]\n",
    "    if bv >= bn\n",
    "]\n",
    "\n",
    "@triton.autotune(\n",
    "    configs=_fwd_configs,\n",
    "    key=['N', 'V', 'D'],\n",
    ")\n",
    "@triton.jit\n",
    "def _cce_fwd_parallel_kernel(\n",
    "    E_ptr, C_ptr, x_ptr, lse_ptr, logits_corr_ptr, Locks,\n",
    "    stride_E_N, stride_E_D, stride_C_V, stride_C_D,\n",
    "    stride_lse, stride_logits_corr, stride_locks,\n",
    "    N, V, D: tl.constexpr,\n",
    "    BLOCK_N: tl.constexpr, BLOCK_V: tl.constexpr, NUM_LOCKS: tl.constexpr\n",
    "):\n",
    "    pid = tl.program_id(0)\n",
    "    num_pid_n = tl.cdiv(N, BLOCK_N)\n",
    "    pid_n = pid % num_pid_n # do in this order for less contention.\n",
    "    pid_v = pid // num_pid_n\n",
    "\n",
    "    # compute row and vocab blocks\n",
    "    row_ids = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
    "    mask_n = row_ids < N\n",
    "    col_ids = pid_v * BLOCK_V + tl.arange(0, BLOCK_V)\n",
    "    mask_v = col_ids < V\n",
    "\n",
    "    # accumulate dot products over D dimension\n",
    "    accum = tl.zeros((BLOCK_N, BLOCK_V), dtype=tl.float32)\n",
    "    for d in range(D):\n",
    "        # load E[:,d] and C[:,d]\n",
    "        e_d = tl.load(E_ptr + row_ids * stride_E_N + d * stride_E_D, mask=mask_n, other=0.0)\n",
    "        c_d = tl.load(C_ptr + col_ids * stride_C_V + d * stride_C_D, mask=mask_v, other=0.0)\n",
    "        accum += e_d[:, None] * c_d[None, :]\n",
    "\n",
    "    logits = tl.where(mask_v[None, :], accum, -float('inf'))\n",
    "    block_max = tl.max(logits, axis=1)\n",
    "    logits_stable = logits - block_max[:, None]\n",
    "    sumexp = tl.sum(tl.exp(logits_stable), axis=1)\n",
    "    block_lse = block_max + tl.log(sumexp)\n",
    "\n",
    "    # correct logit extraction\n",
    "    tgt = tl.load(x_ptr + row_ids, mask=mask_n, other=0)\n",
    "    in_block = mask_n & (tgt >= pid_v * BLOCK_V) & (tgt < pid_v * BLOCK_V + BLOCK_V)\n",
    "    corr_mask = col_ids[None, :] == tgt[:, None]\n",
    "    corr_vals = tl.sum(tl.where(corr_mask, logits, 0.0), axis=1)\n",
    "\n",
    "    # atomic accumulation of LSE using locks\n",
    "    for i in range(BLOCK_N):\n",
    "        r = pid_n * BLOCK_N + i\n",
    "        if r < N:\n",
    "            lock_id = (r * NUM_LOCKS) // N\n",
    "            lock_ptr = Locks + lock_id * stride_locks\n",
    "            while tl.atomic_cas(lock_ptr, 0, 1) == 1:\n",
    "                pass\n",
    "            old = tl.load(lse_ptr + r * stride_lse)\n",
    "            mask_i = tl.arange(0, BLOCK_N) == i\n",
    "            block_lse_i = tl.sum(tl.where(mask_i, block_lse, 0.0))\n",
    "            new_max = old if old > block_lse_i else block_lse_i\n",
    "            new_val = new_max + tl.log(tl.exp(old - new_max) + tl.exp(block_lse_i - new_max))\n",
    "            tl.store(lse_ptr + r * stride_lse, new_val)\n",
    "            tl.atomic_xchg(lock_ptr, 0)\n",
    "\n",
    "    # store correct logit\n",
    "    tl.store(logits_corr_ptr + row_ids * stride_logits_corr, corr_vals, mask=in_block)\n",
    "\n",
    "# --- Autotune configs for backward kernel ---\n",
    "_bwd_configs = [\n",
    "    triton.Config({'BLOCK_V': bv}, num_warps=nw, num_stages=1)\n",
    "    for bv in [64, 128, 256, 512]\n",
    "    for nw in [2, 4, 8]\n",
    "]\n",
    "\n",
    "@triton.autotune(\n",
    "    configs=_bwd_configs,\n",
    "    key=['N', 'V', 'D'],\n",
    ")\n",
    "@triton.jit\n",
    "def _cce_bwd_kernel(\n",
    "    E_ptr, C_ptr, x_ptr, lse_ptr, dloss_ptr,\n",
    "    dE_ptr, dC_ptr,\n",
    "    stride_E_N, stride_E_D,\n",
    "    stride_C_V, stride_C_D,\n",
    "    stride_dE_N, stride_dE_D,\n",
    "    stride_dC_V, stride_dC_D,\n",
    "    N, V, D: tl.constexpr,\n",
    "    BLOCK_V: tl.constexpr\n",
    "):\n",
    "    row = tl.program_id(0)\n",
    "    block_id = tl.program_id(1)\n",
    "    start_v = block_id * BLOCK_V\n",
    "    offs_v = start_v + tl.arange(0, BLOCK_V)\n",
    "    mask_v = offs_v < V\n",
    "\n",
    "    # load embedding e_i\n",
    "    e = tl.load(E_ptr + row * stride_E_N + tl.arange(0, D) * stride_E_D)\n",
    "    # load C_block\n",
    "    C_block = tl.load(\n",
    "        C_ptr\n",
    "        + offs_v[:, None] * stride_C_V\n",
    "        + tl.arange(0, D)[None, :] * stride_C_D,\n",
    "        mask=mask_v[:, None]\n",
    "    )\n",
    "    # load lse\n",
    "    lse = tl.load(lse_ptr + row)\n",
    "\n",
    "    # compute logits_block\n",
    "    logits = tl.sum(C_block * e[None, :], axis=1)\n",
    "    # compute softmax\n",
    "    p = tl.exp(logits - lse)\n",
    "    # gradient w.r.t. logits: p_j - 1 for j == target\n",
    "    tgt = tl.load(x_ptr + row)\n",
    "    mask_target = offs_v == tgt\n",
    "    grad_logits = p - mask_target.to(tl.float32)\n",
    "\n",
    "    # load incoming gradient for this row (scalar)\n",
    "    grad_loss = tl.load(dloss_ptr + row)\n",
    "    coeff = grad_loss / N\n",
    "    grad_logits = grad_logits * coeff\n",
    "\n",
    "    # update dE: sum_v grad_logits[v] * C_block[v]\n",
    "    grad_e = tl.sum(grad_logits[:, None] * C_block, axis=0)\n",
    "    # atomic add to dE[row]\n",
    "    ptr_e = dE_ptr + row * stride_dE_N + tl.arange(0, D) * stride_dE_D\n",
    "    tl.atomic_add(ptr_e, grad_e)\n",
    "\n",
    "    # update dC_block: for each v in block, add grad_logits[v] * e\n",
    "    grad_c = grad_logits[:, None] * e[None, :]\n",
    "    ptr_c = (\n",
    "        dC_ptr\n",
    "        + offs_v[:, None] * stride_dC_V\n",
    "        + tl.arange(0, D)[None, :] * stride_dC_D\n",
    "    )\n",
    "    tl.atomic_add(ptr_c, grad_c)\n",
    "\n",
    "class TritonCCE(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, E, C, x, block_size=128, block_n=16, num_locks=128):\n",
    "        # E: [N,D], C: [V,D], x: [N]\n",
    "        assert E.dim() == 2 and C.dim() == 2\n",
    "        N, D = E.shape\n",
    "        V, D_ = C.shape\n",
    "        assert D == D_\n",
    "        # allocate intermediates\n",
    "        lse = torch.full((N,), -float('inf'), dtype=E.dtype, device=E.device)\n",
    "        logits_corr = torch.zeros((N,), dtype=E.dtype, device=E.device)\n",
    "        # launch new parallel forward kernel\n",
    "        locks = torch.zeros((num_locks,), dtype=torch.uint32, device=E.device)\n",
    "        grid = (triton.cdiv(N, block_n) * triton.cdiv(V, block_size),)\n",
    "        _cce_fwd_parallel_kernel[grid](\n",
    "            E, C, x, lse, logits_corr, locks,\n",
    "            E.stride(0), E.stride(1),\n",
    "            C.stride(0), C.stride(1),\n",
    "            lse.stride(0), logits_corr.stride(0), locks.stride(0),\n",
    "            N, V, D,\n",
    "            BLOCK_N=block_n, BLOCK_V=block_size, NUM_LOCKS=num_locks\n",
    "        )\n",
    "        # compute final loss\n",
    "        loss = (lse - logits_corr).mean()\n",
    "        ctx.save_for_backward(E, C, x, lse)\n",
    "        ctx.block_size = block_size\n",
    "        ctx.block_n = block_n\n",
    "        ctx.num_locks = num_locks\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_loss):\n",
    "        E, C, x, lse = ctx.saved_tensors\n",
    "        N, D = E.shape\n",
    "        V, _ = C.shape\n",
    "        # alloc grads\n",
    "        dE = torch.zeros_like(E)\n",
    "        dC = torch.zeros_like(C)\n",
    "        # prepare a vector of perâ€row grad_loss entries\n",
    "        grad_vec = grad_loss.expand(N).contiguous()\n",
    "        # launch backward kernel\n",
    "        grid = (N, triton.cdiv(V, ctx.block_size))\n",
    "        _cce_bwd_kernel[grid](\n",
    "            E, C, x, lse, grad_vec,\n",
    "            dE, dC,\n",
    "            E.stride(0), E.stride(1),\n",
    "            C.stride(0), C.stride(1),\n",
    "            dE.stride(0), dE.stride(1),\n",
    "            dC.stride(0), dC.stride(1),\n",
    "            N, V, D, BLOCK_V=ctx.block_size\n",
    "        )\n",
    "        return dE, dC, None, None\n",
    "\n",
    "# Convenience module\n",
    "def triton_cut_cross_entropy(E, C, x, block_size=64, block_n=16, num_locks=128):\n",
    "    return TritonCCE.apply(E, C, x, block_size, block_n, num_locks)\n",
    "\n",
    "# Numerical correctness tests for CCE kernel\n",
    "def test_forward():\n",
    "    # setup parameters\n",
    "    N = 2048\n",
    "    V = 50000\n",
    "    D = 1024\n",
    "    block_size = 256\n",
    "    block_n = 16\n",
    "    num_locks = 128\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    E = torch.randn(N, D, device=device, dtype=torch.float32, requires_grad=True)\n",
    "    C = torch.randn(V, D, device=device, dtype=torch.float32, requires_grad=True)\n",
    "    x = torch.randint(0, V, (N,), device=device, dtype=torch.long)\n",
    "    # reference loss\n",
    "    logits = E @ C.t()\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "    ref_loss = loss_fn(logits, x)\n",
    "    # triton loss\n",
    "    tri_loss = triton_cut_cross_entropy(E, C, x, block_size, block_n, num_locks)\n",
    "    assert torch.isclose(ref_loss, tri_loss, atol=1e-5), f\"Forward test failed: ref={ref_loss.item()}, tri={tri_loss.item()}\"\n",
    "    print(f\"Forward test passed: {ref_loss.item():.6f} vs {tri_loss.item():.6f}\")\n",
    "\n",
    "def test_backward():\n",
    "    # setup parameters\n",
    "    N = 2048\n",
    "    V = 50000\n",
    "    D = 1024\n",
    "    block_size = 256\n",
    "    block_n = 16\n",
    "    num_locks = 128\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    E = torch.randn(N, D, device=device, dtype=torch.float32, requires_grad=True)\n",
    "    C = torch.randn(V, D, device=device, dtype=torch.float32, requires_grad=True)\n",
    "    x = torch.randint(0, V, (N,), device=device, dtype=torch.long)\n",
    "    # reference backward\n",
    "    logits = E @ C.t()\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "    ref_loss = loss_fn(logits, x)\n",
    "    ref_loss.backward()\n",
    "    ref_dE = E.grad.clone()\n",
    "    ref_dC = C.grad.clone()\n",
    "    # triton backward\n",
    "    E.grad = None\n",
    "    C.grad = None\n",
    "    tri_loss = triton_cut_cross_entropy(E, C, x, block_size, block_n, num_locks)\n",
    "    tri_loss.backward()\n",
    "    tri_dE = E.grad.clone()\n",
    "    tri_dC = C.grad.clone()\n",
    "    # compare gradients\n",
    "    assert torch.allclose(ref_dE, tri_dE, atol=1e-5), f\"Backward test dE failed: max diff={(ref_dE - tri_dE).abs().max().item()}\"\n",
    "    assert torch.allclose(ref_dC, tri_dC, atol=1e-5), f\"Backward test dC failed: max diff={(ref_dC - tri_dC).abs().max().item()}\"\n",
    "    print(\"Backward test passed.\")\n",
    "\n",
    "# Simple benchmark against vanilla torch\n",
    "configs = [\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=[\"V\"],\n",
    "        x_vals=[1024, 2048, 4096, 8192, 16384, 32768, 50000],\n",
    "        line_arg=\"provider\",\n",
    "        line_vals=[\"Torch\", \"TritonCCE\"],\n",
    "        line_names=[\"Torch\", \"TritonCCE\"],\n",
    "        styles=[(\"red\", \"-\"), (\"blue\", \"-\")],\n",
    "        ylabel=\"Time (ms)\",\n",
    "        plot_name=\"CCE Benchmark\",\n",
    "        args={\"N\": 2048, \"D\": 1024, \"block_size\": 1024, \"block_n\": 16, \"num_locks\": 128},\n",
    "    )\n",
    "]\n",
    "@triton.testing.perf_report(configs)\n",
    "def benchmark(V, provider, N, D, block_size, block_n, num_locks):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    E = torch.randn(N, D, device=device, dtype=torch.float32)\n",
    "    C = torch.randn(V, D, device=device, dtype=torch.float32)\n",
    "    x = torch.randint(0, V, (N,), device=device, dtype=torch.long)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "    if provider == \"Torch\":\n",
    "        result = triton.testing.do_bench(lambda: loss_fn(E @ C.t(), x))\n",
    "    else:\n",
    "        result = triton.testing.do_bench(lambda: triton_cut_cross_entropy(E, C, x, block_size, block_n, num_locks))\n",
    "    if isinstance(result, tuple):\n",
    "        ms, min_ms, max_ms = result\n",
    "    else:\n",
    "        ms = min_ms = max_ms = result\n",
    "    return ms, min_ms, max_ms\n",
    "\n",
    "def measure_time_and_memory(fn, device):\n",
    "    # reset PyTorch's peak-mem counter\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "    torch.cuda.synchronize(device)\n",
    "    # measure time\n",
    "    ms = triton.testing.do_bench(fn)\n",
    "    torch.cuda.synchronize(device)\n",
    "    # read back the high-water mark (in bytes)\n",
    "    peak = torch.cuda.max_memory_allocated(device)\n",
    "    return ms, peak\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_forward()\n",
    "    test_backward()\n",
    "    benchmark.run(show_plots=True, print_data=True)\n",
    "    N = 16384\n",
    "    D = 4\n",
    "    V = 16384\n",
    "    block_size = 128\n",
    "    block_n = 16\n",
    "    num_locks = 128\n",
    "    # example usage\n",
    "    device = torch.device(\"cuda\")\n",
    "    E = torch.randn(N, D, device=device, requires_grad=True)\n",
    "    C = torch.randn(V, D, device=device, requires_grad=True)\n",
    "    x = torch.randint(0, V, (N,), device=device)\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "    def run_torch():\n",
    "        return loss_fn(E @ C.t(), x)\n",
    "\n",
    "    def run_triton():\n",
    "        return triton_cut_cross_entropy(E, C, x, block_size, block_n, num_locks)\n",
    "\n",
    "    for provider, fn in [(\"Torch\", run_torch), (\"TritonCCE\", run_triton)]:\n",
    "        ms, peak_bytes = measure_time_and_memory(fn, device)\n",
    "        print(f\"{provider:10s}  time: {ms:.2f} ms   peak mem: {peak_bytes/1024**2:.1f} MB\")\n",
    "\n",
    "    # benchmark backward pass\n",
    "    def run_torch_backward():\n",
    "        E.grad = None\n",
    "        C.grad = None\n",
    "        loss = loss_fn(E @ C.t(), x)\n",
    "        loss.backward()\n",
    "\n",
    "    def run_triton_backward():\n",
    "        E.grad = None\n",
    "        C.grad = None\n",
    "        loss = triton_cut_cross_entropy(E, C, x, block_size, block_n, num_locks)\n",
    "        loss.backward()\n",
    "\n",
    "    for provider, fn in [(\"Torch BWD\", run_torch_backward), (\"TritonCCE BWD\", run_triton_backward)]:\n",
    "        ms, peak_bytes = measure_time_and_memory(fn, device)\n",
    "        print(f\"{provider:10s}  time: {ms:.2f} ms   peak mem: {peak_bytes/1024**2:.1f} MB\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "um chat its slow but holy memory gains\n"
     ]
    }
   ],
   "source": [
    "print(\"um chat its slow but holy memory gains\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaL4",
   "dataSources": [
    {
     "databundleVersionId": 11802066,
     "sourceId": 91496,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
